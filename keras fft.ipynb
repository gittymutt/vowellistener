{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/deploying-a-simple-machine-learning-model-into-a-webapp-using-tensorflow-js-3609c297fb04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 15 12:22:16 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 451.67       Driver Version: 451.67       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 105... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P8    N/A /  N/A |     75MiB /  4096MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1680    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      4368    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      5472    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A      6676    C+G   ...wekyb3d8bbwe\\Music.UI.exe    N/A      |\n",
      "|    0   N/A  N/A      9120    C+G   ...ekyb3d8bbwe\\commsapps.exe    N/A      |\n",
      "|    0   N/A  N/A     10264    C+G   ...\\atom\\app-1.53.0\\atom.exe    N/A      |\n",
      "|    0   N/A  N/A     11960    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13540    C+G   ...4__8j3eq9eme6ctt\\IGCC.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the dataset       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "path = 'fft_data'\n",
    "with open(path + '/ee_fft_data.txt', 'r') as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "obj = json.loads(data)\n",
    "\n",
    "\n",
    "for fft in obj:\n",
    "   dataset.append((fft['vowel'], np.array(fft['FFT'], dtype=np.uint8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'fft_data'\n",
    "with open(path + '/ih_fft_data.txt', 'r') as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "obj = json.loads(data)\n",
    "\n",
    "\n",
    "for fft in obj:\n",
    "   dataset.append((fft['vowel'], np.array(fft['FFT'], dtype=np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dataset = [vowel for vowel, fft in dataset]\n",
    "x_dataset = [fft for vowel, fft in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to numpy arrays and reshape training data for tf\n",
    "x_dataset = np.array(x_dataset)\n",
    "x_dataset = x_dataset.reshape(len(x_dataset),512,1).astype('float32')\n",
    "\n",
    "y_dataset = np.array(y_dataset).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in to train and test\n",
    "cutoff = round(len(y_dataset)*0.8) # 80/20 split\n",
    "X_train = x_dataset[:cutoff]\n",
    "y_train = y_dataset[:cutoff]\n",
    "X_test = x_dataset[cutoff:]\n",
    "y_test = y_dataset[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 154, 38, 192)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_dataset), len(X_train), len(x_test), len(X_train)+len(X_test)\n",
    "len(y_dataset), len(y_train), len(y_test), len(y_train)+len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.],\n",
       "       [ 56.],\n",
       "       [130.],\n",
       "       [155.],\n",
       "       [146.],\n",
       "       [132.],\n",
       "       [158.],\n",
       "       [157.],\n",
       "       [120.],\n",
       "       [120.],\n",
       "       [127.],\n",
       "       [ 99.],\n",
       "       [ 79.],\n",
       "       [ 97.],\n",
       "       [ 81.],\n",
       "       [ 34.],\n",
       "       [  5.],\n",
       "       [  1.],\n",
       "       [ 72.],\n",
       "       [101.],\n",
       "       [100.],\n",
       "       [ 69.],\n",
       "       [ 12.],\n",
       "       [ 15.],\n",
       "       [ 21.],\n",
       "       [ 10.],\n",
       "       [ 29.],\n",
       "       [ 40.],\n",
       "       [ 29.],\n",
       "       [  0.],\n",
       "       [ 10.],\n",
       "       [  0.],\n",
       "       [ 22.],\n",
       "       [ 15.],\n",
       "       [  9.],\n",
       "       [ 18.],\n",
       "       [  6.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 11.],\n",
       "       [  8.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 10.],\n",
       "       [  0.],\n",
       "       [  4.],\n",
       "       [  0.],\n",
       "       [ 42.],\n",
       "       [ 83.],\n",
       "       [ 87.],\n",
       "       [ 41.],\n",
       "       [ 85.],\n",
       "       [ 98.],\n",
       "       [ 77.],\n",
       "       [ 93.],\n",
       "       [111.],\n",
       "       [ 94.],\n",
       "       [ 16.],\n",
       "       [ 54.],\n",
       "       [ 46.],\n",
       "       [ 33.],\n",
       "       [ 76.],\n",
       "       [ 77.],\n",
       "       [ 18.],\n",
       "       [ 83.],\n",
       "       [ 96.],\n",
       "       [ 74.],\n",
       "       [ 71.],\n",
       "       [ 95.],\n",
       "       [ 81.],\n",
       "       [ 85.],\n",
       "       [110.],\n",
       "       [103.],\n",
       "       [ 65.],\n",
       "       [ 93.],\n",
       "       [ 98.],\n",
       "       [ 69.],\n",
       "       [ 13.],\n",
       "       [ 18.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  7.],\n",
       "       [ 14.],\n",
       "       [  7.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  7.],\n",
       "       [  0.],\n",
       "       [  3.],\n",
       "       [ 15.],\n",
       "       [ 10.],\n",
       "       [  0.],\n",
       "       [  4.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  3.],\n",
       "       [  4.],\n",
       "       [  0.],\n",
       "       [  6.],\n",
       "       [ 14.],\n",
       "       [  7.],\n",
       "       [ 24.],\n",
       "       [ 35.],\n",
       "       [ 20.],\n",
       "       [  0.],\n",
       "       [  4.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 15.],\n",
       "       [ 13.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  7.],\n",
       "       [ 18.],\n",
       "       [ 29.],\n",
       "       [ 25.],\n",
       "       [ 12.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 11.],\n",
       "       [ 16.],\n",
       "       [ 12.],\n",
       "       [ 10.],\n",
       "       [ 17.],\n",
       "       [ 12.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  6.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  7.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  5.],\n",
       "       [  3.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  4.],\n",
       "       [  9.],\n",
       "       [  7.],\n",
       "       [ 10.],\n",
       "       [  9.],\n",
       "       [  3.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 10.],\n",
       "       [  7.],\n",
       "       [  0.],\n",
       "       [ 17.],\n",
       "       [  4.],\n",
       "       [  3.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [ 17.],\n",
       "       [ 27.],\n",
       "       [ 33.],\n",
       "       [ 27.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 21.],\n",
       "       [  4.],\n",
       "       [ 10.],\n",
       "       [ 21.],\n",
       "       [ 17.],\n",
       "       [  5.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 14.],\n",
       "       [ 16.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 17.],\n",
       "       [ 10.],\n",
       "       [ 21.],\n",
       "       [ 26.],\n",
       "       [ 30.],\n",
       "       [ 27.],\n",
       "       [ 21.],\n",
       "       [  6.],\n",
       "       [  0.],\n",
       "       [ 14.],\n",
       "       [ 21.],\n",
       "       [ 17.],\n",
       "       [  5.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 19.],\n",
       "       [ 19.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 14.],\n",
       "       [  6.],\n",
       "       [  3.],\n",
       "       [  5.],\n",
       "       [  4.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  6.],\n",
       "       [ 14.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  4.],\n",
       "       [ 20.],\n",
       "       [  6.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 11.],\n",
       "       [ 23.],\n",
       "       [ 28.],\n",
       "       [ 20.],\n",
       "       [ 13.],\n",
       "       [ 15.],\n",
       "       [ 24.],\n",
       "       [ 22.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  8.],\n",
       "       [ 11.],\n",
       "       [  7.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 20.],\n",
       "       [ 30.],\n",
       "       [ 18.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  9.],\n",
       "       [ 18.],\n",
       "       [ 22.],\n",
       "       [ 24.],\n",
       "       [ 12.],\n",
       "       [  0.],\n",
       "       [  9.],\n",
       "       [ 10.],\n",
       "       [  2.],\n",
       "       [  1.],\n",
       "       [ 17.],\n",
       "       [ 26.],\n",
       "       [ 19.],\n",
       "       [  0.],\n",
       "       [  4.],\n",
       "       [  5.],\n",
       "       [  7.],\n",
       "       [ 10.],\n",
       "       [ 17.],\n",
       "       [ 16.],\n",
       "       [  0.],\n",
       "       [ 17.],\n",
       "       [  5.],\n",
       "       [  0.],\n",
       "       [  8.],\n",
       "       [  6.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  3.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 11.],\n",
       "       [  9.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  8.],\n",
       "       [  5.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  3.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  2.],\n",
       "       [  2.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  5.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 10.],\n",
       "       [ 19.],\n",
       "       [ 18.],\n",
       "       [ 14.],\n",
       "       [  5.],\n",
       "       [  3.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 11.],\n",
       "       [  0.],\n",
       "       [ 24.],\n",
       "       [ 34.],\n",
       "       [ 31.],\n",
       "       [ 14.],\n",
       "       [  4.],\n",
       "       [ 14.],\n",
       "       [  2.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  4.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  7.],\n",
       "       [  2.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [  7.],\n",
       "       [ 14.],\n",
       "       [  5.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  9.],\n",
       "       [ 12.],\n",
       "       [  5.],\n",
       "       [  6.],\n",
       "       [ 24.],\n",
       "       [ 21.],\n",
       "       [  4.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  9.],\n",
       "       [  5.],\n",
       "       [  0.],\n",
       "       [  6.],\n",
       "       [ 16.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  3.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  9.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  5.],\n",
       "       [ 15.],\n",
       "       [ 12.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [ 15.],\n",
       "       [ 10.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  4.],\n",
       "       [  5.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.],\n",
       "       [  0.]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "#num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "\n",
    "#X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "#X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "#y_train = np_utils.to_categorical(y_train)\n",
    "#y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 0.6675 - accuracy: 0.6169\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 0.6183 - accuracy: 0.6558\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5609 - accuracy: 0.6688\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 0s 915us/step - loss: 0.4997 - accuracy: 0.7078\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.7273\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3786 - accuracy: 0.7857\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 0s 938us/step - loss: 0.3277 - accuracy: 0.9091\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.9740\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 0s 938us/step - loss: 0.2443 - accuracy: 1.0000\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.2110 - accuracy: 1.0000\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 1.0000\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 1.0000\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 1.0000\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 0s 936us/step - loss: 0.1090 - accuracy: 1.0000\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.0904 - accuracy: 1.0000\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 0s 938us/step - loss: 0.0740 - accuracy: 1.0000\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 0s 998us/step - loss: 0.0614 - accuracy: 1.0000\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.0505 - accuracy: 1.0000\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 0s 998us/step - loss: 0.0420 - accuracy: 1.0000\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 0s 999us/step - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 0s 877us/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 0s 876us/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 0s 873us/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 0s 938us/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 0s 999us/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 0s 876us/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 1.00 - 0s 938us/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 0s 943us/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 0s 935us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 0s 876us/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 0s 999us/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 0s 938us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 0s 873us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 0s 876us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.00 - 0s 750us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 0s 714us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 0s 703us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 0s 937us/step - loss: 9.9510e-04 - accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 0s 937us/step - loss: 9.6063e-04 - accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 9.2769e-04 - accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 8.9881e-04 - accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 8.6990e-04 - accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8.4490e-04 - accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 8.1804e-04 - accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 7.9433e-04 - accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7.7115e-04 - accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 7.4822e-04 - accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 7.2750e-04 - accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7.0745e-04 - accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 6.8575e-04 - accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 6.6805e-04 - accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6.4828e-04 - accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 6.3125e-04 - accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 6.1416e-04 - accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5.9755e-04 - accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5.8232e-04 - accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 5.6732e-04 - accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 5.5127e-04 - accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 0s 811us/step - loss: 5.3775e-04 - accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5.2407e-04 - accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5.1071e-04 - accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 0s 752us/step - loss: 4.9875e-04 - accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 4.8550e-04 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4.7359e-04 - accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4.6125e-04 - accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4.4974e-04 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4.3890e-04 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 0s 751us/step - loss: 4.2859e-04 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4.1781e-04 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4.0810e-04 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3.9961e-04 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 0s 811us/step - loss: 3.9011e-04 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3.8152e-04 - accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 3.7277e-04 - accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 0s 875us/step - loss: 3.6450e-04 - accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 0s 810us/step - loss: 3.5663e-04 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "16/16 [==============================] - 0s 811us/step - loss: 3.4888e-04 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3.4123e-04 - accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3.3460e-04 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 3.2708e-04 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 3.2037e-04 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "16/16 [==============================] - 0s 814us/step - loss: 3.1382e-04 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "16/16 [==============================] - 0s 748us/step - loss: 3.0726e-04 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3.0113e-04 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "16/16 [==============================] - 0s 811us/step - loss: 2.9464e-04 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2.8849e-04 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2.8238e-04 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "16/16 [==============================] - 0s 811us/step - loss: 2.7684e-04 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "16/16 [==============================] - 0s 751us/step - loss: 2.7127e-04 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2.6577e-04 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2.6066e-04 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 2.5588e-04 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2.5093e-04 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2.4611e-04 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "16/16 [==============================] - 0s 873us/step - loss: 2.4141e-04 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "16/16 [==============================] - 0s 814us/step - loss: 2.3715e-04 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "16/16 [==============================] - 0s 873us/step - loss: 2.3275e-04 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "16/16 [==============================] - 0s 813us/step - loss: 2.2821e-04 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 2.2434e-04 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 2.2017e-04 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 2.1621e-04 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 2.1251e-04 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2.0876e-04 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "16/16 [==============================] - 0s 814us/step - loss: 2.0502e-04 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "16/16 [==============================] - 0s 815us/step - loss: 2.0103e-04 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "16/16 [==============================] - 0s 752us/step - loss: 1.9752e-04 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "16/16 [==============================] - 0s 811us/step - loss: 1.9385e-04 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "16/16 [==============================] - 0s 812us/step - loss: 1.9029e-04 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 1.8704e-04 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 1.8372e-04 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "16/16 [==============================] - 0s 811us/step - loss: 1.8051e-04 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "16/16 [==============================] - 0s 750us/step - loss: 1.7730e-04 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "16/16 [==============================] - 0s 687us/step - loss: 1.7436e-04 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "16/16 [==============================] - 0s 811us/step - loss: 1.7146e-04 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "16/16 [==============================] - 0s 748us/step - loss: 1.6847e-04 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "16/16 [==============================] - 0s 748us/step - loss: 1.6565e-04 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "16/16 [==============================] - 0s 748us/step - loss: 1.6287e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17395cacdc0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=512, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 793us/step - loss: 1.6116e-04 - accuracy: 1.0000\n",
      "Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9987996e-01, 2.4953762e-05],\n",
       "       [9.9999869e-01, 1.0827419e-07],\n",
       "       [9.9986660e-01, 3.1765674e-05],\n",
       "       [1.6348783e-06, 9.9999797e-01],\n",
       "       [1.1570745e-04, 9.9995208e-01],\n",
       "       [2.8888213e-05, 9.9997747e-01],\n",
       "       [2.9806502e-05, 9.9998045e-01],\n",
       "       [9.9944556e-01, 1.8560886e-04],\n",
       "       [2.5016214e-05, 9.9998188e-01],\n",
       "       [4.7727253e-06, 9.9999511e-01]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fft_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-15 20:37:58.332109: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-11-15 20:37:58.332146: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter --input_format keras fft_model.h5 ..\\..\\javascript\\model_fft_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 9497-8F11\n",
      "\n",
      " Directory of C:\\Users\\pedro\\Documents\\programming\\python\\tf_fft\n",
      "\n",
      "11/15/2020  08:34 PM    <DIR>          .\n",
      "11/15/2020  08:34 PM    <DIR>          ..\n",
      "11/15/2020  01:26 PM    <DIR>          .ipynb_checkpoints\n",
      "11/15/2020  01:42 PM    <DIR>          fft_data\n",
      "11/15/2020  08:31 PM           106,328 fft_model.h5\n",
      "11/15/2020  01:15 PM    <DIR>          js_model\n",
      "11/15/2020  08:34 PM           287,004 keras fft.ipynb\n",
      "11/15/2020  01:14 PM           768,088 model.h5\n",
      "11/15/2020  01:09 PM                84 one-hot encoding key.txt\n",
      "               4 File(s)      1,161,504 bytes\n",
      "               5 Dir(s)  202,190,438,400 bytes free\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
